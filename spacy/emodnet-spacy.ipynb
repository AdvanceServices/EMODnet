{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import spacy.cli\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "ruler=nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script in order to convert ubiai IOB format into spacy format input for training\n",
    "#for correct read, delete the first raw of the annotations\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_all = pd.read_csv(\"training_IOBall.tsv\", sep='\\t+', header=None, engine='python') #skiprows=1,\n",
    "dev_all = pd.read_csv(\"development_IOBall.tsv\", sep='\\t+', header=None, engine='python')\n",
    "eval_all = pd.read_csv(\"evaluation_IOBall.tsv\", sep='\\t+', header=None, engine='python')\n",
    "\n",
    "def iob_to_spacy(df):\n",
    "    words = df[0]\n",
    "    tags = df[1]\n",
    "    total_words = len(words)\n",
    "    data = []\n",
    "    data_i = 0\n",
    "    i = 0\n",
    "\n",
    "    sentence = \"\"\n",
    "    sentence_len = 0\n",
    "    sentence_entities = []\n",
    "    for word in words:\n",
    "        dot_index = word.find('.')\n",
    "        if (dot_index != -1):\n",
    "            # found in dot_index position\n",
    "            if (dot_index == len(word)-1):\n",
    "                # last char\n",
    "                # check the 1st letter in the next word\n",
    "                if (total_words-1 > i):\n",
    "                    # exists in next word\n",
    "                    # check if the first letter is capital \n",
    "                    if (words[i+1][0].isupper()):\n",
    "                        sentence += word\n",
    "                        if (tags[i] != 'O'):\n",
    "                            entity = (sentence_len, len(sentence), tags[i])\n",
    "                            sentence_entities.append(entity)\n",
    "                        sentence_len = len(sentence)\n",
    "                        new_val = (sentence, {\"entities\": sentence_entities})\n",
    "                        data.append(new_val)\n",
    "                        sentence = \"\"\n",
    "                        sentence_len = 0\n",
    "                        sentence_entities = []\n",
    "                    else:\n",
    "                        sentence += word + \" \"\n",
    "                        if (tags[i] != 'O'):\n",
    "                            entity = (sentence_len, len(sentence), tags[i])\n",
    "                            sentence_entities.append(entity)\n",
    "                        sentence_len = len(sentence)\n",
    "        else:\n",
    "            sentence += word + \" \"\n",
    "            if (tags[i] != 'O'):\n",
    "                entity = (sentence_len, len(sentence), tags[i])\n",
    "                sentence_entities.append(entity)\n",
    "            sentence_len = len(sentence)\n",
    "        i += 1\n",
    "    for sent in data:\n",
    "        entities = sent[1]['entities']\n",
    "        new_entities = []\n",
    "        entity_start = 0\n",
    "        entity_end = 0\n",
    "        entity_type = \"\"\n",
    "        for entity in entities:\n",
    "            if entity[2][0] == \"B\":\n",
    "                if entity_end != 0:\n",
    "                    new_entity = (entity_start, entity_end, entity_type)\n",
    "                    new_entities.append(new_entity)\n",
    "                entity_start = entity[0]\n",
    "                entity_end = entity[1]\n",
    "                entity_type = entity[2]\n",
    "            else:\n",
    "                entity_end = entity[1]\n",
    "        if entity_end != 0:\n",
    "            new_entity = (entity_start, entity_end, entity_type)\n",
    "            new_entities.append(new_entity)\n",
    "            sent[1]['entities']=new_entities\n",
    "\n",
    "    for sent in data:\n",
    "        entity_start = 0\n",
    "        entity_end = 0\n",
    "        entity_type = \"\"\n",
    "        new_entities=[]\n",
    "        for entity in sent[1]['entities']:\n",
    "            entity_start = entity[0]\n",
    "            entity_end = entity[1]\n",
    "            entity_type = entity[2]\n",
    "            entity_type=re.sub(r'.', '', entity_type , count = 2)\n",
    "            new_entity = (entity_start, entity_end, entity_type)\n",
    "            new_entities.append(new_entity)\n",
    "            sent[1]['entities']=new_entities\n",
    "    return data\n",
    "\n",
    "train_fulltext=iob_to_spacy(train_all)\n",
    "dev_fulltext=iob_to_spacy(dev_all)\n",
    "eval_fulltext=iob_to_spacy(eval_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionaries\n",
    "df=pd.read_csv(\"dictionaries-annot\\Distribution_descriptors.csv\",header=None)\n",
    "df1=pd.read_csv(\"dictionaries-annot\\Life_stages.csv\",header=None)\n",
    "df2=pd.read_csv(\"dictionaries-annot\\Body_size.csv\",header=None)\n",
    "df3=pd.read_csv(\"dictionaries-annot\\Sampling_devices.csv\",header=None,encoding='cp1252')\n",
    "\n",
    "df_distr_descr=df.iloc[:,0]\n",
    "df_life_stages=df1.iloc[:,0]\n",
    "df_body_size=df2.iloc[:,0]\n",
    "df_sampl_devices=df3.iloc[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to create custom ner model\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "#adding labels-entities to ner \n",
    "ner.add_label(\"DISTRIBUTION_DESCRIPTOR\")\n",
    "ner.add_label(\"LIFE_STAGE\")\n",
    "ner.add_label(\"BODY_SIZE\")\n",
    "ner.add_label(\"SAMPLING_DEVICE\")\n",
    "ner.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "#converting data into .spacy file\n",
    "def training_data_format(train_data,name):\n",
    "    doc_bin = DocBin()\n",
    "    for training_example in tqdm(train_data): \n",
    "        text = training_example[0]\n",
    "        # print(text)\n",
    "        labels = training_example[1]['entities']\n",
    "        # print(training_example[1])\n",
    "        doc = nlp.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in labels:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        filtered_ents = filter_spans(ents)\n",
    "        doc.ents = filtered_ents \n",
    "        doc_bin.add(doc)\n",
    "    \n",
    "    return(doc_bin.to_disk(\"train_\" + name + \".spacy\"))\n",
    "\n",
    "\n",
    "training_data_format(train_fulltext,\"train_full\")\n",
    "training_data_format(dev_fulltext,\"dev_full\")\n",
    "training_data_format(eval_fulltext,\"eval_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration file initiallization\n",
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' #for error fixing\n",
    "\n",
    "#training the model\n",
    "!python -m spacy train config.cfg --output ./ --paths.train ./train_train_full.spacy --paths.dev ./train_eval_full.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "#load trained model\n",
    "nlp_full=spacy.load(\"model-bestfulltxt\")\n",
    "doc=nlp_full(\"Atherinids are small marine, estuarine and freshwater fishes not exceeding 120 mm SL (a soon to be described species of Craterocephalus may reach 300 mm SL), occurring predominantly in the Old World, with only Alepidomus evermanni (freshwa\u0002ters of Cuba) and two marine species, Atheri\u0002nomorus stipes and Hypoatherina harringtonensis (predominantly in the shore waters of the Caribbean) known from the New World. I\")\n",
    "\n",
    "print([(ent.text, ent.label_ ,ent.start_char, ent.end_char, ent.ent_id_) for ent in doc.ents])\n",
    "print(\"\\n\")\n",
    "\n",
    "displacy.render(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "#in order to put the entity_ruler into nlp_ner pipeline\n",
    "#entity ruler's patterns(entities) evaluate only if they are not annotaded in training data, so the ”entity_ruler” will only add new entities that match to the patterns only if they don’t overlap with existing entities predicted by the statistical model\n",
    "def entity_ruler(nlp_model,model):\n",
    "    if \"entity_ruler\" not in nlp_model.pipe_names:\n",
    "        ruler=nlp_model.add_pipe(\"entity_ruler\")\n",
    "    else:\n",
    "        ruler=nlp_model.get_pipe(\"entity_ruler\")\n",
    "\n",
    "    #FOR PATTERNS\n",
    "    def dict_func(df,linkdf,label):\n",
    "        patterns=[]\n",
    "        j=0\n",
    "        for i in df:\n",
    "            dict={\"label\": label}\n",
    "            dict[\"pattern\"]=[{\"LOWER\" : str.lower(i)}]\n",
    "            # dict[\"id\"]=linkdf[[1]][1][j]\n",
    "            patterns.append(dict)\n",
    "            tokens=tokenizer(i)\n",
    "            if len(tokens) == 2:\n",
    "                dict={\"label\": label}\n",
    "                dict[\"pattern\"]=[{\"LOWER\" : str.lower(str(tokens[0]))}, {\"IS_PUNCT\": True}, {\"LOWER\" : str.lower(str(tokens[1]))}]\n",
    "                # dict[\"id\"]=linkdf[[1]][1][j]\n",
    "                patterns.append(dict)\n",
    "            j=j+1\n",
    "        ruler.add_patterns(patterns)\n",
    "\n",
    "    dict_func(df_distr_descr,df,\"DISTRIBUTION_DESCRIPTOR\")\n",
    "    dict_func(df_life_stages,df1,\"LIFE_STAGE\")\n",
    "    dict_func(df_body_size,df2,\"BODY_SIZE\")\n",
    "    dict_func(df_sampl_devices,df3,\"SAMPLING_DEVICE\")\n",
    "\n",
    "    #in order to put entity ruler into the trained model pipeline\n",
    "    nlp_model.to_disk(model)\n",
    "\n",
    "nlp_full=spacy.load(\"model-bestfulltxt\")\n",
    "entity_ruler(nlp_full,\"model-bestfulltxt_ruler\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation of trained model\n",
    "!python -m spacy evaluate model-bestfulltxt_ruler/ train_eval_full.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trained model\n",
    "\n",
    "nlp_ner=spacy.load(\"model-bestfulltxt\")\n",
    "nlp_ner.pipe_names\n",
    "\n",
    "#load trained model with entity ruler\n",
    "\n",
    "nlp_ner_ruler=spacy.load(\"model-bestfulltxt_ruler\")\n",
    "nlp_ner_ruler.pipe_names\n",
    "\n",
    "paper2=nlp_ner(\"They are inhabitants of shallow reef areas, usually encountered in less than 10 m depth. During the day they are mainly sedentary, frequently seen resting on the bottom under rock or coral outcrops on substrata containing substantial amounts of sand, silt, mud, or algae.\")\n",
    "doc=nlp_ner_ruler(\"They are inhabitants of shallow reef areas, usually encountered in less than 10 m depth. During the day they are mainly sedentary, frequently seen resting on the bottom under rock or coral outcrops on substrata containing substantial amounts of sand, silt, mud, or algae.\")\n",
    "\n",
    "print([(ent.text, ent.label_ ,ent.start_char, ent.end_char, ent.ent_id_) for ent in doc.ents])\n",
    "print(\"\\n\")\n",
    "print([(ent.text, ent.label_ ,ent.start_char, ent.end_char, ent.ent_id_) for ent in paper2.ents])\n",
    "\n",
    "displacy.render(paper2, style=\"ent\" )\n",
    "displacy.render(doc, style=\"ent\" )\n",
    "nlp_ner_ruler.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #combining 2 ner from trained models (roberta and spacy model)\n",
    "\n",
    "# # nlp_roberta=spacy.load(\"model-roberta_dev_ruler\")\n",
    "# # nlp_roberta.rename_pipe(\"ner\", \"ner_roberta\")\n",
    "# # nlp_roberta.to_disk(\"model-roberta_dev_ruler\")\n",
    "\n",
    "# nlp1 = spacy.load(\"model-corpusIOB_dev_ruler\")\n",
    "\n",
    "# # Load the second model\n",
    "# nlp2 = spacy.load(\"model-roberta_dev_ruler\")\n",
    "# # Add the components from the second model to the first\n",
    "# for name, component in nlp2.pipeline:\n",
    "#     if name  not in nlp1.pipe_names:\n",
    "#         nlp1.add_pipe(name=name,source=nlp2,factory_name=name)\n",
    "    \n",
    "# nlp1.to_disk(\"combined_IOBspacy_roberta_dev_ruler\")\n",
    "\n",
    "# nlp1=spacy.load(\"combined_IOBspacy_roberta_dev_ruler\")\n",
    "# doc = nlp1(\"Most marine fish and invertebrate species produce free and small early-stages which are part of the plankton. These incompletely developed individuals are highly vulnerable to unsuitable conditions like starvation and environmental variability, and it was early recognized that survival during these stages often regulates recruitment and adult population size (Cowan and Shaw, 2002, Pineda et al., 2007). Recruitment theories have thus focused on the environmental modulation of larval survival, and they generally assume that while spawning occurs within relatively fixed time-frames along the year cycle, hydrographic conditions and plankton production show higher inter-annual variability.\")\n",
    "\n",
    "# displacy.render(doc, style=\"ent\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94e7e65be05b4ed6ba93e18d7d6886bab5ac9fc35747516c0d44d14f8ddf2d8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
